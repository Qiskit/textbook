{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instances and extensions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chapter will cover several quantum variational algorithms, including:\n",
    "\n",
    "* [Variational Quantum Eigensolver (VQE)](https://arxiv.org/abs/1304.3061)\n",
    "* [Subspace Search VQE (SSVQE)](https://arxiv.org/abs/1810.09434)\n",
    "* [Variational Quantum Deflation (VQD)](https://arxiv.org/abs/1805.08138)\n",
    "* [Quantum Sampling Regression (QSR)](https://arxiv.org/pdf/2012.02338)\n",
    "\n",
    "By using these algorithms, we will learn about several design ideas that can be incorporated into custom variational algorithms, such as weights, penalties, oversampling, and undersampling. We encourage you to experiment with these concepts and share your findings with the community."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Quantum Eigensolver (VQE)\n",
    "\n",
    "[VQE](https://arxiv.org/abs/1304.3061) is one of the most widely used variational quantum algorithms, setting up a template for other algorithms to build upon. \n",
    "\n",
    "![VQE](images/instances_VQE.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VQE's layout is simple:\n",
    "\n",
    "- Prepare reference operators $U_R$\n",
    "  - We start from the state $|0\\rangle$ and go to the reference state $|\\rho\\rangle$\n",
    "- Apply the variational form $U_V(\\vec\\theta_{i,j})$ to create an ansatz $U_A(\\vec\\theta_{i,j})$\n",
    "  - We go from the state $|\\rho\\rangle$ to $U_V(\\vec\\theta_{i,j})|\\rho\\rangle = |\\psi(\\vec\\theta_{i,j})\\rangle$\n",
    "- Bootstrap at $i=0$ if we have a similar problem (typically found via classical simulation or sampling)\n",
    "  - Each optimizer will be bootstrapped differently, resulting in an initial set of parameter vectors $\\Theta_0 := \\\\{ {\\vec\\theta_{0,j} | j \\in \\mathcal{J}_\\text{opt}^0} \\\\}$ (e.g., from an initial point $\\vec\\theta_0$).\n",
    "- Evaluate the cost function $C(\\vec\\theta_{i,j}) := \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$ for all prepared states on a quantum computer.\n",
    "- Use a classical optimizer to select the next set of parameters $\\Theta_{i+1}$.\n",
    "- Repeat the process until convergence is reached.\n",
    "\n",
    "This is a simple classical optimization loop where we evaluate the cost function. Some optimizers may require multiple evaluations to calculate a gradient, determine the next iteration, or assess convergence.\n",
    "\n",
    "Here's the example for the following observable:\n",
    "\n",
    "$$\n",
    "\\hat{O}_1 = 2 II - 2 XX + 3 YY - 3 ZZ,\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "uses-hardware"
    ]
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import TwoLocal\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit_ibm_runtime import QiskitRuntimeService, Estimator\n",
    "import numpy as np\n",
    "\n",
    "# Add your token below\n",
    "service = QiskitRuntimeService(\n",
    "    channel=\"ibm_quantum\",\n",
    ")\n",
    "\n",
    "def cost_function_vqe(theta):\n",
    "    observable = SparsePauliOp.from_list([(\"II\", 2), (\"XX\", -2), (\"YY\", 3), (\"ZZ\", -3)])\n",
    "    reference_circuit = QuantumCircuit(2)\n",
    "    reference_circuit.x(0)\n",
    "\n",
    "    variational_form = TwoLocal(\n",
    "        2,\n",
    "        rotation_blocks=[\"rz\", \"ry\"],\n",
    "        entanglement_blocks=\"cx\",\n",
    "        entanglement=\"linear\",\n",
    "        reps=1,\n",
    "    )\n",
    "    ansatz = reference_circuit.compose(variational_form)\n",
    "\n",
    "    backend = service.backend(\"ibmq_qasm_simulator\")\n",
    "    \n",
    "    # Use estimator to get the expected values corresponding to each ansatz\n",
    "    estimator = Estimator(session=backend)\n",
    "    job = estimator.run(ansatz, observable, theta)\n",
    "    values = job.result().values\n",
    "\n",
    "    return values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this cost function to calculate optimal parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qiskit.algorithms.optimizers import COBYLA\n",
    "\n",
    "initial_theta = np.ones(8)\n",
    "optimizer = COBYLA()\n",
    "\n",
    "optimizer_result = optimizer.minimize(fun=cost_function_vqe, x0=initial_theta)\n",
    "\n",
    "optimal_parameters = optimizer_result.x\n",
    "print(optimal_parameters)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use our optimal parameters to calculate our minimum eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observable = SparsePauliOp.from_list([(\"II\", 2), (\"XX\", -2), (\"YY\", 3), (\"ZZ\", -3)])\n",
    "reference_circuit = QuantumCircuit(2)\n",
    "reference_circuit.x(0)\n",
    "\n",
    "variational_form = TwoLocal(\n",
    "    2,\n",
    "    rotation_blocks=[\"rz\", \"ry\"],\n",
    "    entanglement_blocks=\"cx\",\n",
    "    entanglement=\"linear\",\n",
    "    reps=1,\n",
    ")\n",
    "ansatz = reference_circuit.compose(variational_form)\n",
    "solution = ansatz.bind_parameters(optimal_parameters)\n",
    "\n",
    "backend = service.backend(\"ibmq_qasm_simulator\")\n",
    "estimator = Estimator(session=backend)\n",
    "job = estimator.run(solution, observable)\n",
    "values = job.result().values\n",
    "\n",
    "experimental_min_eigenvalue = values[0]\n",
    "print(experimental_min_eigenvalue)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subspace Search VQE (SSVQE)\n",
    "\n",
    "[SSVQE](https://arxiv.org/abs/1810.09434) is a variant of VQE that allows obtaining the first $k$ eigenvalues of an observable $\\hat{H}$ with eigenvalues {$\\lambda_0, \\lambda_1,...,\\lambda_{N-1}$}, where $N\\geq k$. Without loss of generality, we assume that $\\lambda_0<\\lambda_1<...<\\lambda_{N-1}$. SSQVE introduces a new idea by adding weights to help prioritize optimizing for the term with the largest weight.\n",
    "\n",
    "![SSVQE](images/instances_SSVQE.png)\n",
    "\n",
    "To implement this algorithm, we need $k$ mutually orthogonal reference states `{latex} \\{ |\\rho_j\\rangle \\}_{j=0}^{k-1}`, meaning $\\langle \\rho_j | \\rho_l \\rangle = \\delta_{jl}$ for $j,l<k$. These states can be constructed using Pauli operators. The cost function of this algorithm is then:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C(\\vec{\\theta}) \n",
    "\n",
    "& := \\sum_{j=0}^{k-1} w_j \\langle \\rho_j | U_{V}^{\\dagger}(\\vec{\\theta})\\hat{H} U_{V}(\\vec{\\theta})|\\rho_j \\rangle \\\\[1mm]\n",
    "\n",
    "& := \\sum_{j=0}^{k-1} w_j \\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle \\\\[1mm]\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $w_j$ is an arbitrary positive number such that if $j<l<k$ then $w_j>w_l$, and $U_V(\\vec{\\theta})$ is the user-defined variational form.\n",
    "\n",
    "The SSVQE algorithm relies on the fact that eigenstates corresponding to different eigenvalues are mutually orthogonal. Specifically, the inner product of $U_V(\\vec{\\theta})|\\rho_j\\rangle$ and $U_V(\\vec{\\theta})|\\rho_l\\rangle$ can be expressed as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\langle \\rho_j | U_{V}^{\\dagger}(\\vec{\\theta})U_{V}(\\vec{\\theta})|\\rho_l \\rangle\n",
    "\n",
    "& = \\langle \\rho_j | I |\\rho_l \\rangle \\\\[1mm]\n",
    "\n",
    "& = \\langle \\rho_j | \\rho_l \\rangle \\\\[1mm]\n",
    "\n",
    "& = \\delta_{jl}\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The first equality holds because $U_{V}(\\vec{\\theta})$ is a quantum operator and is therefore unitary. The last equality holds because of the orthogonality of the reference states $|\\rho_j\\rangle$. The fact that orthogonality is preserved through unitary transformations is deeply related to the principle of conservation of information, as expressed in quantum information science. Under this view, non-unitary transformations represent processes where information is either lost or injected.\n",
    "\n",
    "Weights $w_j$ help ensure that all the states are eigenstates. If the weights are sufficiently different, the term with the largest weight (i.e., $w_0$) will be given priority during optimization over the others. As a result, the resulting state $U_{V}(\\vec{\\theta})|\\rho_0 \\rangle$ will become the eigenstate corresponding to $\\lambda_0$. Because `{latex} \\{ U_{V}(\\vec{\\theta})|\\rho_j\\rangle \\}_{j=0}^{k-1}` are mutually orthogonal, the remaining states will be orthogonal to it and, therefore, contained in the subspace corresponding to the eigenvalues {$\\lambda_1,...,\\lambda_{N-1}$}.\n",
    "\n",
    "Applying the same argument to the rest of the terms, the next priority would then be the term with weight $w_1$, so $U_{V}(\\vec{\\theta})|\\rho_1 \\rangle$ would be the eigenstate corresponding to $\\lambda_1$, and the other terms would be contained in the eigenspace of {$\\lambda_2,...,\\lambda_{N-1}$}.\n",
    "\n",
    "By reasoning inductively, we deduce that $U_{V}(\\vec{\\theta})|\\rho_j \\rangle$ will be an approximate eigenstate of $\\lambda_j$ for $0\\leq j < k$.\n",
    "\n",
    "SSVQE's can be summarized as follows:\n",
    "\n",
    "- Prepare several reference states by applying a unitary U_R to k different computational basis states\n",
    "   - This algorithm requires the usage of $k$ mutually orthogonal reference states `{latex} \\{ |\\rho_j\\rangle \\}_{j=0}^{k-1}`, such that $\\langle \\rho_j | \\rho_l \\rangle = \\delta_{jl}$ for $j,l<k$.\n",
    "- Apply the variational form $U_V(\\vec\\theta_{i,j})$ to each reference state, resulting in the following ansatz $U_A(\\vec\\theta_{i,j})$.\n",
    "- Bootstrap at $i=0$ if a similar problem is available (usually found via classical simulation or sampling).\n",
    "- Evaluate the cost function $C(\\vec\\theta_{i,j}) := \\sum_{j=0}^{k-1} w_j \\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle$ for all prepared states on a quantum computer.\n",
    "  - This can be separated into calculating the expectation value for an observable $\\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle$ and multiplying that result by $w_j$.\n",
    "  - Afterward, the cost function returns the sum of all weighted expectation values.\n",
    "- Use a classical optimizer to determine the next set of parameters $\\Theta_{i+1}$.\n",
    "- Repeat the above steps until convergence is achieved.\n",
    "\n",
    "You will be re-constructing SSVQE's cost function in the assessment!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Quantum Deflation (VQD)\n",
    "\n",
    "[VQD](https://arxiv.org/abs/1805.08138) is an iterative method that extends VQE to obtain the first $k$ eigenvalues of an observable $\\hat{H}$ with eigenvalues {$\\lambda_0, \\lambda_1,...,\\lambda_{N-1}$}, where $N\\geq k$, instead of only the first. For the rest of this section, we will assume, without loss of generality, that $\\lambda_0\\leq\\lambda_1\\leq...\\leq\\lambda_{N-1}$. VQD introduces the notion of a penalty cost to guide the optimization process.\n",
    "\n",
    "![VQD](images/instances_VQD.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VQD introduces a penalty term, denoted as $\\beta$, to balance the contribution of each overlap term to the cost. This penalty term serves to penalize the optimization process if orthogonality is not achieved. We impose this constraint because the eigenstates of an observable, or a Hermitian operator, corresponding to different eigenvalues are always mutually orthogonal, or can be made to be so in the case of degeneracy or repeated eigenvalues. Thus, by enforcing orthogonality with the eigenstate corresponding to $\\lambda_0$, we are effectively optimizing over the subspace that corresponds to the rest of the eigenvalues {$\\lambda_1, \\lambda_2,..., \\lambda_{N-1}$}. Here, $\\lambda_1$ is the lowest eigenvalue from the rest of the eigenvalues and, therefore, the optimal solution of the new problem can be obtained using the variational theorem.\n",
    "\n",
    "The general idea behind VQD is to use VQE as usual to obtain the lowest eigenvalue $\\lambda_0 := C_0(\\vec\\theta^0) \\equiv C_\\text{VQE}(\\vec\\theta^0)$ along with the corresponding (approximate) eigenstate $|\\psi(\\vec{\\theta^0})\\rangle$ for some optimal parameter vector $\\vec{\\theta^0}$. Then, to obtain the next eigenvalue $\\lambda_1 > \\lambda_0$, instead of minimizing the cost function $C_0(\\vec{\\theta}) := \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$, we optimize:\n",
    "\n",
    "$$\n",
    "C_1(\\vec{\\theta}) := \n",
    "C_0(\\vec{\\theta})+ \\beta_0 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^0})\\rangle  |^2 \n",
    "$$\n",
    "\n",
    "The positive value $\\beta_0$ should ideally be greater than $\\lambda_1-\\lambda_0$. \n",
    "\n",
    "This introduces a new cost function that can be viewed as a constrained problem, where we minimize $C_\\text{VQE}(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$ subject to the constraint that the state must be orthogonal to the previously obtained $|\\psi(\\vec{\\theta^0})\\rangle$, with $\\beta_0$ acting as a penalty term if the constraint is not satisfied.\n",
    "\n",
    "Alternatively, this new problem can be interpreted as running VQE on the new observable:\n",
    "\n",
    "$$\n",
    "\\hat{H_1} := \\hat{H} + \\beta_0 |\\psi(\\vec{\\theta^0})\\rangle \\langle \\psi(\\vec{\\theta^0})|\n",
    "\\quad \\Rightarrow \\quad \n",
    "C_1(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H_1} | \\psi(\\vec{\\theta})\\rangle,\n",
    "$$\n",
    "\n",
    "Assuming that the solution to the new problem is $|\\psi(\\vec{\\theta^1})\\rangle$, the expected value of $\\hat{H}$ (not $\\hat{H_1}$) should be $ \\langle \\psi(\\vec{\\theta^1}) | \\hat{H} | \\psi(\\vec{\\theta^1})\\rangle = \\lambda_1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain the third eigenvalue $\\lambda_2$, the cost function to optimize is:\n",
    "\n",
    "$$\n",
    "C_2(\\vec{\\theta}) := \n",
    "C_1(\\vec{\\theta}) + \\beta_1 |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^1})\\rangle  |^2 \n",
    "$$\n",
    "\n",
    "where $\\beta_1$ is a positive constant large enough to enforce orthogonality of the solution state to both $|\\psi(\\vec{\\theta^0})\\rangle$ and $|\\psi(\\vec{\\theta^1})\\rangle$. This penalizes states in the search space that do not meet this requirement, effectively restricting the search space. Thus, the optimal solution of the new problem should be the eigenstate corresponding to $\\lambda_2$.\n",
    "\n",
    "Like the previous case, this new problem can also be interpreted as VQE with the observable:\n",
    "\n",
    "$$\n",
    "\\hat{H_2} := \\hat{H_1} + \\beta_1 |\\psi(\\vec{\\theta^1})\\rangle \\langle \\psi(\\vec{\\theta^1})|\n",
    "\\quad \\Rightarrow \\quad \n",
    "C_2(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H_2} | \\psi(\\vec{\\theta})\\rangle.\n",
    "$$\n",
    "\n",
    "If the solution to this new problem is $|\\psi(\\vec{\\theta^2})\\rangle$, the expected value of $\\hat{H}$ (not $\\hat{H_2}$) should be $ \\langle \\psi(\\vec{\\theta^2}) | \\hat{H} | \\psi(\\vec{\\theta^2})\\rangle = \\lambda_2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogously, to obtain the $k$-th eigenvalue $\\lambda_{k-1}$, you would minimize the cost function:\n",
    "\n",
    "$$\n",
    "C_{k-1}(\\vec{\\theta}) := \n",
    "C_{k-2}(\\vec{\\theta}) + \\beta_{k-2} |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^{k-2}})\\rangle  |^2,\n",
    "$$\n",
    "\n",
    "Remember that we defined $\\vec{\\theta^j}$ such that $\\langle \\psi(\\vec{\\theta^j}) | \\hat{H} | \\psi(\\vec{\\theta^j})\\rangle = \\lambda_j, \\forall j<k$. This problem is equivalent to minimizing $C(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle$ but with the constraint that the state must be orthogonal to $|\\psi(\\vec{\\theta^j})\\rangle ; \\forall j \\in {0, \\cdots, k-1}$, thereby restricting the search space to the subspace corresponding to the eigenvalues {$\\lambda_{k-1},\\cdots,\\lambda_{N-1}$}.\n",
    "\n",
    "This problem is equivalent to a VQE with the observable:\n",
    "\n",
    "$$\n",
    "\\hat{H}_{k-1} := \n",
    "\\hat{H}_{k-2} + \\beta_{k-2} |\\psi(\\vec{\\theta^{k-2}})\\rangle \\langle \\psi(\\vec{\\theta^{k-2}})|\n",
    "\\quad \\Rightarrow \\quad \n",
    "C_{k-1}(\\vec{\\theta}) = \\langle \\psi(\\vec{\\theta}) | \\hat{H}_{k-1} | \\psi(\\vec{\\theta})\\rangle,\n",
    "$$\n",
    "\n",
    "As you can see from the process, to obtain the $k$-th eigenvalue, you need the (approximate) eigenstates of the previous $k-1$ eigenvalues, so you would need to run VQE a total of $k$ times. Therefore, VQD's cost function is as follows:\n",
    "\n",
    "$$\n",
    "C_k(\\vec{\\theta}) =\n",
    "\\langle \\psi(\\vec{\\theta}) | \\hat{H} | \\psi(\\vec{\\theta})\\rangle +\n",
    "\\sum_{j=0}^{k-1}\\beta_j |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^j})\\rangle |^2\n",
    "$$\n",
    "\n",
    "VQD's layout can be summarized as follows:\n",
    "\n",
    "- Prepare a reference operator $U_R$\n",
    "- Apply variational form $U_V(\\vec\\theta_{i,j})$ to the reference state, creating the following ansatze $U_A(\\vec\\theta_{i,j})$\n",
    "- Bootstrap at $i=0$ if we have a similar problem (typically found via classical simulation or sampling).\n",
    "- Evaluate the cost function $C_k(\\vec{\\theta})$, which involves computing $k$ excited states and an array of $\\beta$'s defining the overlap penalty for each overlap term.\n",
    "  - Calculate the expectation value for an observable $\\langle \\psi_{j}(\\vec{\\theta}) | \\hat{H} | \\psi_{j}(\\vec{\\theta}) \\rangle$ for each $k$\n",
    "  - Calculate the penalty $\\sum_{j=0}^{k-1}\\beta_j |\\langle \\psi(\\vec{\\theta})| \\psi(\\vec{\\theta^j})\\rangle |^2$.\n",
    "  - The cost function should then return the sum of these two terms\n",
    "- Use a classical optimizer to choose the next set of parameters $\\Theta_{i+1}$.\n",
    "- Repeat this process until convergence is reached."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Sampling Regression (QSR)\n",
    "\n",
    "One of the main issues with VQE is the multiple calls to a quantum computer that are required to obtain the parameters for each step, including $k$, $k-1$, etc. This is especially problematic when access to quantum devices is queued. While a [`Session`](https://qiskit.org/documentation/partners/qiskit_ibm_runtime/how_to/run_session.html) can be used to group multiple iterative calls, an alternative approach is to use sampling. By utilizing more classical resources, we can complete the full optimization process in a single call. This is where [Quantum Sampling Regression](https://arxiv.org/pdf/2012.02338) comes into play. Since access to quantum computers is still a low-offer/high-demand commodity, we find this trade-off to be both possible and convenient for many current studies. This approach harnesses all available classical capabilities while still capturing many of the inner workings and intrinsic properties of quantum computations that do not appear in simulation.\n",
    "\n",
    "![QSR](images/instances_QSR.png)\n",
    "\n",
    "The idea behind QSR is that the cost function $C(\\theta) := \\langle \\psi(\\theta) | \\hat{H} | \\psi(\\theta)\\rangle$ can be expressed as a Fourier series in the following manner:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "C(\\vec{\\theta}) \n",
    "\n",
    "& := \\langle \\psi(\\theta) | \\hat{H} | \\psi(\\theta)\\rangle \\\\[1mm]\n",
    "\n",
    "& := a_0 + \\sum_{k=1}^S[a_k\\cos(k\\theta)+ b_k\\sin(k\\theta)] \\\\[1mm]\n",
    "\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Depending on the periodicity and bandwidth of the original function, the set $S$ may be finite or infinite. For the purposes of this discussion, we will assume it is infinite. The next step is to sample the cost function $C(\\theta)$ multiple times in order to obtain the Fourier coefficients $\\{a_0, a_k, b_k\\}_{k=1}^S$. Specifically, since we have $2S+1$ unknowns, we will need to sample the cost function $2S+1$ times.\n",
    "\n",
    "If we then sample the cost function for $2S+1$ parameter values {$\\theta_1,...,\\theta_{2S+1}$}, we can obtain the following system:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} 1 & \\cos(\\theta_1) & \\sin(\\theta_1) & \\cos(2\\theta_1) & ... & \\sin(S\\theta_1) \\\\\n",
    "1 & \\cos(\\theta_2) & \\sin(\\theta_2) & \\cos(2\\theta_2) & \\cdots & \\sin(S\\theta_2)\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\ \n",
    "1 & \\cos(\\theta_{2S+1}) & \\sin(\\theta_{2S+1}) & \\cos(2\\theta_{2S+1}) & \\cdots & \\sin(S\\theta_{2S+1})\n",
    "\\end{pmatrix} \\begin{pmatrix} a_0 \\\\ a_1 \\\\ b_1 \\\\ a_2 \\\\ \\vdots \\\\ b_S \\end{pmatrix} = \\begin{pmatrix} C(\\theta_1) \\\\ C(\\theta_2) \\\\ \\vdots \\\\ C(\\theta_{2S+1}) \\end{pmatrix},\n",
    "$$\n",
    "\n",
    "that we'll rewrite as \n",
    "\n",
    "$$\n",
    "Fa=c.\n",
    "$$\n",
    "\n",
    "In practice, this system is generally not consistent because the cost function values $c$ are not exact. Therefore, it is usually a good idea to normalize them by multiplying them by $F^\\dagger$ on the left, which results in:\n",
    "\n",
    "$$\n",
    "F^\\dagger Fa = F^\\dagger c.\n",
    "$$\n",
    "\n",
    "This new system is always consistent, and its solution is a least-squares solution to the original problem. If we have $k$ parameters instead of just one, and each parameter $\\theta^i$ has its own $S_i$ for $i \\in {1,...,k}$, then the total number of samples required is:\n",
    "\n",
    "$$\n",
    "T=\\prod_{i=1}^k(2S_i+1)\\leq \\prod_{i=1}^k(2S_{max}+1) = (2S_{max}+1)^n,\n",
    "$$\n",
    "\n",
    "where $S_{\\max} = \\max_i(S_i)$. Furthermore, adjusting $S_{\\max}$ as a tunable parameter (instead of inferring it) opens up new possibilities, such as:\n",
    "\n",
    "- **Oversampling**: to improve accuracy.\n",
    "- **Undersampling**: to boost performance by reducing runtime overhead or eliminating local minima.\n",
    "\n",
    "\n",
    "QSR's layout can be summarized as follows:\n",
    "\n",
    "- Prepare reference operators $U_R$\n",
    "  - We'll go from the state $|0\\rangle$ to the reference state $|\\rho\\rangle$\n",
    "- Apply the  variational form $U_V(\\vec\\theta_{i,j})$ to create an ansatz $U_A(\\vec\\theta_{i,j})$\n",
    "  - Determine the bandwidth associated with each parameter in the ansatz. An upper bound is sufficient.\n",
    "- Bootstrap at $i=0$ if we have a similar problem (typically found via classical simulation or sampling)\n",
    "- Sample the cost function $C(\\vec\\theta) := a_0 + \\sum_{k=1}^S[a_k\\cos(k\\theta)+ b_k\\sin(k\\theta)]$ at least $T$ times\n",
    "  - $T=\\prod_{i=1}^k(2S_i+1)\\leq \\prod_{i=1}^k(2S_{max}+1) = (2S_{max}+1)^n$\n",
    "  - Decide whether to oversample/undersample to balance speed vs accuracy by adjusting $T$.\n",
    "- Compute the Fourier coefficients from the samples (i.e., solve the normalized linear system of equations).\n",
    "- Solve for the global minimum of the resulting regression function on a classical machine."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this lesson, you learned about multiple variational instances available:\n",
    "\n",
    "- General layout\n",
    "- Introducing weights and penalties to adjust a cost function\n",
    "- Exploring undersampling vs oversampling to trade-off speed vs accuracy\n",
    "\n",
    "These ideas can be adapted to form a custom variational algorithm that fits your problem. We encourage you to share your results with the community. The next lesson will explore how to use a variational algorithm to solve an application."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
